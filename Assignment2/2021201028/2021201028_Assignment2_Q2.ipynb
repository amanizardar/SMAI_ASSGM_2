{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "improving-pepper",
   "metadata": {},
   "source": [
    "# Assignment 2 - Question 2\n",
    "The objective of this assignment is to get you familiarize with  the  problem  of  `Linear Regression`.\n",
    "\n",
    "## Instructions\n",
    "- Write your code and analysis in the indicated cells.\n",
    "- Ensure that this notebook runs without errors when the cells are run in sequence.\n",
    "- Do not attempt to change the contents of other cells.\n",
    "- No inbuilt functions to be used until specified\n",
    "\n",
    "## Submission\n",
    "- Ensure that this notebook runs without errors when the cells are run in sequence.\n",
    "- Rename the notebook to `<roll_number>_Assignment2_Q2.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "final-transaction",
   "metadata": {},
   "source": [
    "## 2.0 Background about the dataset\n",
    "\n",
    "TLDR: You have 4 independent variables (`float`) for each molecule. You can use a linear combination of these 4 independent variables to predict the bandgap (dependent variable) of each molecule.\n",
    "\n",
    "You can read more about the problem in [Li et al, Bandgap tuning strategy by cations and halide ions of lead halide perovskites learned from machine learning, RSC Adv., 2021,11, 15688-15694](https://doi.org/10.1039/D1RA03117A)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "lyric-olympus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hundred-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_molecules = list()\n",
    "\n",
    "with open('bg_data.txt', 'r') as infile:\n",
    "    input_rows = csv.DictReader(infile)\n",
    "    \n",
    "    for row in input_rows:\n",
    "        current_mol = ([float(row['Cs']), float(row['FA']), float(row['Cl']), float(row['Br'])], float(row['Bandgap']))\n",
    "        all_molecules.append(current_mol)\n",
    "\n",
    "random.shuffle(all_molecules)\n",
    "\n",
    "\n",
    "num_train = int(len(all_molecules) * 0.8)\n",
    "\n",
    "# each point in x_train has 4 values - 1 for each feature\n",
    "x_train = [x[0] for x in all_molecules[:num_train]]\n",
    "# each point in y_train has 1 value - the bandgap of the molecule\n",
    "y_train = [x[1] for x in all_molecules[:num_train]]\n",
    "\n",
    "x_test = [x[0] for x in all_molecules[num_train:]]\n",
    "y_test = [x[1] for x in all_molecules[num_train:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cd754dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(all_molecules)\n",
    "# num_train\n",
    "# all_molecules[:num_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-direction",
   "metadata": {},
   "source": [
    "### 2.1 Implement a Linear Regression model that minimizes the MSE **without using any libraries**. You may use NumPy to vectorize your code, but *do not use numpy.polyfit* or anything similar.\n",
    "\n",
    "2.1.1 Explain how you plan to implement Linear Regression in 5-10 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a736c9d2",
   "metadata": {},
   "source": [
    "Linear Regression finds a relationship between one or more feature variables and a continuous target variable.\n",
    "I implemented Linear Regression using a class and using gradient decent to find the variable which best fits to my training data.\n",
    "I have taken learning rate as 0.046 as it was giving minimum MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frozen-forth",
   "metadata": {},
   "source": [
    "<!-- your answer to 1.1.1 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-winter",
   "metadata": {},
   "source": [
    "2.1.2 Implement Linear Regression using `x_train` and `y_train` as the train dataset.\n",
    "\n",
    "2.1.2.1 Choose the best learning rate and print the learning rate for which you achieved the best MSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fe00e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning Rate =  0.046\n"
     ]
    }
   ],
   "source": [
    "print(\"Learning Rate = \",0.046)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "angry-depression",
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement Linear Regression\n",
    "x_train=np.array(x_train)\n",
    "class LinearRegression() :\n",
    "    def __init__( self, learning_rate=0.046, n_iterations=900) :\n",
    "        self.learning_rate =  learning_rate\n",
    "        self.n_iterations=n_iterations\n",
    "\n",
    "    def fit(self,x,y):\n",
    "\n",
    "        self.cost_ = []\n",
    "        # w = np.zeros((x_train, 1))\n",
    "        # len(x_train)\n",
    "\n",
    "        # x_train.shape\n",
    "        m,n = x.shape\n",
    "        self.w=np.zeros(n)\n",
    "\n",
    "        for i in range(self.n_iterations):\n",
    "            y_pred = np.dot(x,self.w)\n",
    "            res = y_pred-y_train\n",
    "            grad_vec = np.dot(x_train.T,res)\n",
    "            self.w-=(self.learning_rate/m)*grad_vec\n",
    "            cost = np.sum((res**2))/(2*m)\n",
    "            self.cost_.append(cost)\n",
    "        return self\n",
    "\n",
    "    def predict(self ,x):\n",
    "        return np.dot(x,self.w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d68bea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(x_train,y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "# print(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44695079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.498077900358207\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.mean_squared_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "angry-tampa",
   "metadata": {},
   "source": [
    "2.1.3 Make a [Parity Plot](https://en.wikipedia.org/wiki/Parity_plot) of your model's bandgap predictions on the test set with the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "foster-center",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x1440 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the predictions of x_test into `y_pred`\n",
    "\n",
    "#\n",
    "# ...\n",
    "#\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(10,20))\n",
    "\n",
    "ax.scatter(y_test, y_pred)\n",
    "\n",
    "lims = [\n",
    "    np.min([ax.get_xlim(), ax.get_ylim()]),\n",
    "    np.max([ax.get_xlim(), ax.get_ylim()]),\n",
    "]\n",
    "ax.plot(lims, lims, 'k-', alpha=0.75, zorder=0)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlim(lims)\n",
    "ax.set_ylim(lims)\n",
    "\n",
    "ax.set_title('Parity Plot of Custom Linear Regression')\n",
    "ax.set_xlabel('Ground truth bandgap values')\n",
    "ax.set_ylabel('Predicted bandgap values')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominant-chaos",
   "metadata": {},
   "source": [
    "### 2.2 Implement Ridge regression\n",
    "2.2.1 Explain Ridge regression briefly in 1-2 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d38aae",
   "metadata": {},
   "source": [
    "we change cost function of linear regression model to include additional cost for a model that has large cofficient because that is the problem with linear regression as cofficient can be very large. \n",
    "so we include l2 panality which is sum of the squared cofficient.\n",
    "that modified model is called Ridge Regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "happy-cyprus",
   "metadata": {},
   "source": [
    "<!-- Your answer to 1.2.1 -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-forwarding",
   "metadata": {},
   "source": [
    "2.2.2 Implement Ridge regression and make a table of different RMSE scores you achieved with different values of alpha. What does the parameter `alpha` do? How does it affect the results here? Explain in 5-10 lines in total. (You can use scikit-learn from this cell onwards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "violent-northern",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha : 0.000 \t RMSE: 0.024\n",
      "alpha : 0.010 \t RMSE: 0.024\n",
      "alpha : 0.020 \t RMSE: 0.024\n",
      "alpha : 0.030 \t RMSE: 0.024\n",
      "alpha : 0.040 \t RMSE: 0.024\n",
      "alpha : 0.050 \t RMSE: 0.024\n",
      "alpha : 0.060 \t RMSE: 0.023\n",
      "alpha : 0.070 \t RMSE: 0.023\n",
      "alpha : 0.080 \t RMSE: 0.023\n",
      "alpha : 0.090 \t RMSE: 0.023\n",
      "alpha : 0.100 \t RMSE: 0.023\n",
      "alpha : 0.110 \t RMSE: 0.023\n",
      "alpha : 0.120 \t RMSE: 0.023\n",
      "alpha : 0.130 \t RMSE: 0.023\n",
      "alpha : 0.140 \t RMSE: 0.022\n",
      "alpha : 0.150 \t RMSE: 0.022\n",
      "alpha : 0.160 \t RMSE: 0.022\n",
      "alpha : 0.170 \t RMSE: 0.022\n",
      "alpha : 0.180 \t RMSE: 0.022\n",
      "alpha : 0.190 \t RMSE: 0.022\n",
      "alpha : 0.200 \t RMSE: 0.022\n",
      "alpha : 0.210 \t RMSE: 0.022\n",
      "alpha : 0.220 \t RMSE: 0.021\n",
      "alpha : 0.230 \t RMSE: 0.021\n",
      "alpha : 0.240 \t RMSE: 0.021\n",
      "alpha : 0.250 \t RMSE: 0.021\n",
      "alpha : 0.260 \t RMSE: 0.021\n",
      "alpha : 0.270 \t RMSE: 0.021\n",
      "alpha : 0.280 \t RMSE: 0.021\n",
      "alpha : 0.290 \t RMSE: 0.021\n",
      "alpha : 0.300 \t RMSE: 0.021\n",
      "alpha : 0.310 \t RMSE: 0.021\n",
      "alpha : 0.320 \t RMSE: 0.021\n",
      "alpha : 0.330 \t RMSE: 0.021\n",
      "alpha : 0.340 \t RMSE: 0.021\n",
      "alpha : 0.350 \t RMSE: 0.021\n",
      "alpha : 0.360 \t RMSE: 0.021\n",
      "alpha : 0.370 \t RMSE: 0.021\n",
      "alpha : 0.380 \t RMSE: 0.021\n",
      "alpha : 0.390 \t RMSE: 0.021\n",
      "alpha : 0.400 \t RMSE: 0.021\n",
      "alpha : 0.410 \t RMSE: 0.021\n",
      "alpha : 0.420 \t RMSE: 0.021\n",
      "alpha : 0.430 \t RMSE: 0.021\n",
      "alpha : 0.440 \t RMSE: 0.021\n",
      "alpha : 0.450 \t RMSE: 0.021\n",
      "alpha : 0.460 \t RMSE: 0.021\n",
      "alpha : 0.470 \t RMSE: 0.021\n",
      "alpha : 0.480 \t RMSE: 0.021\n",
      "alpha : 0.490 \t RMSE: 0.021\n",
      "alpha : 0.500 \t RMSE: 0.021\n",
      "alpha : 0.510 \t RMSE: 0.021\n",
      "alpha : 0.520 \t RMSE: 0.021\n",
      "alpha : 0.530 \t RMSE: 0.021\n",
      "alpha : 0.540 \t RMSE: 0.021\n",
      "alpha : 0.550 \t RMSE: 0.021\n",
      "alpha : 0.560 \t RMSE: 0.022\n",
      "alpha : 0.570 \t RMSE: 0.022\n",
      "alpha : 0.580 \t RMSE: 0.022\n",
      "alpha : 0.590 \t RMSE: 0.022\n",
      "alpha : 0.600 \t RMSE: 0.022\n",
      "alpha : 0.610 \t RMSE: 0.022\n",
      "alpha : 0.620 \t RMSE: 0.022\n",
      "alpha : 0.630 \t RMSE: 0.022\n",
      "alpha : 0.640 \t RMSE: 0.022\n",
      "alpha : 0.650 \t RMSE: 0.022\n",
      "alpha : 0.660 \t RMSE: 0.023\n",
      "alpha : 0.670 \t RMSE: 0.023\n",
      "alpha : 0.680 \t RMSE: 0.023\n",
      "alpha : 0.690 \t RMSE: 0.023\n",
      "alpha : 0.700 \t RMSE: 0.023\n",
      "alpha : 0.710 \t RMSE: 0.023\n",
      "alpha : 0.720 \t RMSE: 0.023\n",
      "alpha : 0.730 \t RMSE: 0.023\n",
      "alpha : 0.740 \t RMSE: 0.024\n",
      "alpha : 0.750 \t RMSE: 0.024\n",
      "alpha : 0.760 \t RMSE: 0.024\n",
      "alpha : 0.770 \t RMSE: 0.024\n",
      "alpha : 0.780 \t RMSE: 0.024\n",
      "alpha : 0.790 \t RMSE: 0.024\n",
      "alpha : 0.800 \t RMSE: 0.024\n",
      "alpha : 0.810 \t RMSE: 0.024\n",
      "alpha : 0.820 \t RMSE: 0.025\n",
      "alpha : 0.830 \t RMSE: 0.025\n",
      "alpha : 0.840 \t RMSE: 0.025\n",
      "alpha : 0.850 \t RMSE: 0.025\n",
      "alpha : 0.860 \t RMSE: 0.025\n",
      "alpha : 0.870 \t RMSE: 0.025\n",
      "alpha : 0.880 \t RMSE: 0.025\n",
      "alpha : 0.890 \t RMSE: 0.026\n",
      "alpha : 0.900 \t RMSE: 0.026\n",
      "alpha : 0.910 \t RMSE: 0.026\n",
      "alpha : 0.920 \t RMSE: 0.026\n",
      "alpha : 0.930 \t RMSE: 0.026\n",
      "alpha : 0.940 \t RMSE: 0.026\n",
      "alpha : 0.950 \t RMSE: 0.026\n",
      "alpha : 0.960 \t RMSE: 0.027\n",
      "alpha : 0.970 \t RMSE: 0.027\n",
      "alpha : 0.980 \t RMSE: 0.027\n",
      "alpha : 0.990 \t RMSE: 0.027\n",
      "alpha : 1.000 \t RMSE: 0.027\n",
      "alpha : 1.010 \t RMSE: 0.027\n",
      "alpha : 1.020 \t RMSE: 0.027\n",
      "alpha : 1.030 \t RMSE: 0.028\n",
      "alpha : 1.040 \t RMSE: 0.028\n",
      "alpha : 1.050 \t RMSE: 0.028\n",
      "alpha : 1.060 \t RMSE: 0.028\n",
      "alpha : 1.070 \t RMSE: 0.028\n",
      "alpha : 1.080 \t RMSE: 0.028\n",
      "alpha : 1.090 \t RMSE: 0.028\n",
      "alpha : 1.100 \t RMSE: 0.029\n",
      "alpha : 1.110 \t RMSE: 0.029\n",
      "alpha : 1.120 \t RMSE: 0.029\n",
      "alpha : 1.130 \t RMSE: 0.029\n",
      "alpha : 1.140 \t RMSE: 0.029\n",
      "alpha : 1.150 \t RMSE: 0.029\n",
      "alpha : 1.160 \t RMSE: 0.029\n",
      "alpha : 1.170 \t RMSE: 0.030\n",
      "alpha : 1.180 \t RMSE: 0.030\n",
      "alpha : 1.190 \t RMSE: 0.030\n",
      "alpha : 1.200 \t RMSE: 0.030\n",
      "alpha : 1.210 \t RMSE: 0.030\n",
      "alpha : 1.220 \t RMSE: 0.030\n",
      "alpha : 1.230 \t RMSE: 0.030\n",
      "alpha : 1.240 \t RMSE: 0.031\n",
      "alpha : 1.250 \t RMSE: 0.031\n",
      "alpha : 1.260 \t RMSE: 0.031\n",
      "alpha : 1.270 \t RMSE: 0.031\n",
      "alpha : 1.280 \t RMSE: 0.031\n",
      "alpha : 1.290 \t RMSE: 0.031\n",
      "alpha : 1.300 \t RMSE: 0.031\n",
      "alpha : 1.310 \t RMSE: 0.032\n",
      "alpha : 1.320 \t RMSE: 0.032\n",
      "alpha : 1.330 \t RMSE: 0.032\n",
      "alpha : 1.340 \t RMSE: 0.032\n",
      "alpha : 1.350 \t RMSE: 0.032\n",
      "alpha : 1.360 \t RMSE: 0.032\n",
      "alpha : 1.370 \t RMSE: 0.032\n",
      "alpha : 1.380 \t RMSE: 0.033\n",
      "alpha : 1.390 \t RMSE: 0.033\n",
      "alpha : 1.400 \t RMSE: 0.033\n",
      "alpha : 1.410 \t RMSE: 0.033\n",
      "alpha : 1.420 \t RMSE: 0.033\n",
      "alpha : 1.430 \t RMSE: 0.033\n",
      "alpha : 1.440 \t RMSE: 0.033\n",
      "alpha : 1.450 \t RMSE: 0.033\n",
      "alpha : 1.460 \t RMSE: 0.034\n",
      "alpha : 1.470 \t RMSE: 0.034\n",
      "alpha : 1.480 \t RMSE: 0.034\n",
      "alpha : 1.490 \t RMSE: 0.034\n",
      "alpha : 1.500 \t RMSE: 0.034\n",
      "alpha : 1.510 \t RMSE: 0.034\n",
      "alpha : 1.520 \t RMSE: 0.034\n",
      "alpha : 1.530 \t RMSE: 0.034\n",
      "alpha : 1.540 \t RMSE: 0.035\n",
      "alpha : 1.550 \t RMSE: 0.035\n",
      "alpha : 1.560 \t RMSE: 0.035\n",
      "alpha : 1.570 \t RMSE: 0.035\n",
      "alpha : 1.580 \t RMSE: 0.035\n",
      "alpha : 1.590 \t RMSE: 0.035\n",
      "alpha : 1.600 \t RMSE: 0.035\n",
      "alpha : 1.610 \t RMSE: 0.035\n",
      "alpha : 1.620 \t RMSE: 0.036\n",
      "alpha : 1.630 \t RMSE: 0.036\n",
      "alpha : 1.640 \t RMSE: 0.036\n",
      "alpha : 1.650 \t RMSE: 0.036\n",
      "alpha : 1.660 \t RMSE: 0.036\n",
      "alpha : 1.670 \t RMSE: 0.036\n",
      "alpha : 1.680 \t RMSE: 0.036\n",
      "alpha : 1.690 \t RMSE: 0.036\n",
      "alpha : 1.700 \t RMSE: 0.036\n",
      "alpha : 1.710 \t RMSE: 0.037\n",
      "alpha : 1.720 \t RMSE: 0.037\n",
      "alpha : 1.730 \t RMSE: 0.037\n",
      "alpha : 1.740 \t RMSE: 0.037\n",
      "alpha : 1.750 \t RMSE: 0.037\n",
      "alpha : 1.760 \t RMSE: 0.037\n",
      "alpha : 1.770 \t RMSE: 0.037\n",
      "alpha : 1.780 \t RMSE: 0.037\n",
      "alpha : 1.790 \t RMSE: 0.037\n",
      "alpha : 1.800 \t RMSE: 0.038\n",
      "alpha : 1.810 \t RMSE: 0.038\n",
      "alpha : 1.820 \t RMSE: 0.038\n",
      "alpha : 1.830 \t RMSE: 0.038\n",
      "alpha : 1.840 \t RMSE: 0.038\n",
      "alpha : 1.850 \t RMSE: 0.038\n",
      "alpha : 1.860 \t RMSE: 0.038\n",
      "alpha : 1.870 \t RMSE: 0.038\n",
      "alpha : 1.880 \t RMSE: 0.038\n",
      "alpha : 1.890 \t RMSE: 0.039\n",
      "alpha : 1.900 \t RMSE: 0.039\n",
      "alpha : 1.910 \t RMSE: 0.039\n",
      "alpha : 1.920 \t RMSE: 0.039\n",
      "alpha : 1.930 \t RMSE: 0.039\n",
      "alpha : 1.940 \t RMSE: 0.039\n",
      "alpha : 1.950 \t RMSE: 0.039\n",
      "alpha : 1.960 \t RMSE: 0.039\n",
      "alpha : 1.970 \t RMSE: 0.039\n",
      "alpha : 1.980 \t RMSE: 0.040\n",
      "alpha : 1.990 \t RMSE: 0.040\n",
      "alpha : 2.000 \t RMSE: 0.040\n",
      "alpha : 2.010 \t RMSE: 0.040\n",
      "alpha : 2.020 \t RMSE: 0.040\n",
      "alpha : 2.030 \t RMSE: 0.040\n",
      "alpha : 2.040 \t RMSE: 0.040\n",
      "alpha : 2.050 \t RMSE: 0.040\n",
      "alpha : 2.060 \t RMSE: 0.040\n",
      "alpha : 2.070 \t RMSE: 0.040\n",
      "alpha : 2.080 \t RMSE: 0.040\n",
      "alpha : 2.090 \t RMSE: 0.041\n",
      "alpha : 2.100 \t RMSE: 0.041\n",
      "alpha : 2.110 \t RMSE: 0.041\n",
      "alpha : 2.120 \t RMSE: 0.041\n",
      "alpha : 2.130 \t RMSE: 0.041\n",
      "alpha : 2.140 \t RMSE: 0.041\n",
      "alpha : 2.150 \t RMSE: 0.041\n",
      "alpha : 2.160 \t RMSE: 0.041\n",
      "alpha : 2.170 \t RMSE: 0.041\n",
      "alpha : 2.180 \t RMSE: 0.041\n",
      "alpha : 2.190 \t RMSE: 0.042\n",
      "alpha : 2.200 \t RMSE: 0.042\n",
      "alpha : 2.210 \t RMSE: 0.042\n",
      "alpha : 2.220 \t RMSE: 0.042\n",
      "alpha : 2.230 \t RMSE: 0.042\n",
      "alpha : 2.240 \t RMSE: 0.042\n",
      "alpha : 2.250 \t RMSE: 0.042\n",
      "alpha : 2.260 \t RMSE: 0.042\n",
      "alpha : 2.270 \t RMSE: 0.042\n",
      "alpha : 2.280 \t RMSE: 0.042\n",
      "alpha : 2.290 \t RMSE: 0.042\n",
      "alpha : 2.300 \t RMSE: 0.042\n",
      "alpha : 2.310 \t RMSE: 0.043\n",
      "alpha : 2.320 \t RMSE: 0.043\n",
      "alpha : 2.330 \t RMSE: 0.043\n",
      "alpha : 2.340 \t RMSE: 0.043\n",
      "alpha : 2.350 \t RMSE: 0.043\n",
      "alpha : 2.360 \t RMSE: 0.043\n",
      "alpha : 2.370 \t RMSE: 0.043\n",
      "alpha : 2.380 \t RMSE: 0.043\n",
      "alpha : 2.390 \t RMSE: 0.043\n",
      "alpha : 2.400 \t RMSE: 0.043\n",
      "alpha : 2.410 \t RMSE: 0.043\n",
      "alpha : 2.420 \t RMSE: 0.044\n",
      "alpha : 2.430 \t RMSE: 0.044\n",
      "alpha : 2.440 \t RMSE: 0.044\n",
      "alpha : 2.450 \t RMSE: 0.044\n",
      "alpha : 2.460 \t RMSE: 0.044\n",
      "alpha : 2.470 \t RMSE: 0.044\n",
      "alpha : 2.480 \t RMSE: 0.044\n",
      "alpha : 2.490 \t RMSE: 0.044\n",
      "alpha : 2.500 \t RMSE: 0.044\n",
      "alpha : 2.510 \t RMSE: 0.044\n",
      "alpha : 2.520 \t RMSE: 0.044\n",
      "alpha : 2.530 \t RMSE: 0.044\n",
      "alpha : 2.540 \t RMSE: 0.044\n",
      "alpha : 2.550 \t RMSE: 0.045\n",
      "alpha : 2.560 \t RMSE: 0.045\n",
      "alpha : 2.570 \t RMSE: 0.045\n",
      "alpha : 2.580 \t RMSE: 0.045\n",
      "alpha : 2.590 \t RMSE: 0.045\n",
      "alpha : 2.600 \t RMSE: 0.045\n",
      "alpha : 2.610 \t RMSE: 0.045\n",
      "alpha : 2.620 \t RMSE: 0.045\n",
      "alpha : 2.630 \t RMSE: 0.045\n",
      "alpha : 2.640 \t RMSE: 0.045\n",
      "alpha : 2.650 \t RMSE: 0.045\n",
      "alpha : 2.660 \t RMSE: 0.045\n",
      "alpha : 2.670 \t RMSE: 0.045\n",
      "alpha : 2.680 \t RMSE: 0.046\n",
      "alpha : 2.690 \t RMSE: 0.046\n",
      "alpha : 2.700 \t RMSE: 0.046\n",
      "alpha : 2.710 \t RMSE: 0.046\n",
      "alpha : 2.720 \t RMSE: 0.046\n",
      "alpha : 2.730 \t RMSE: 0.046\n",
      "alpha : 2.740 \t RMSE: 0.046\n",
      "alpha : 2.750 \t RMSE: 0.046\n",
      "alpha : 2.760 \t RMSE: 0.046\n",
      "alpha : 2.770 \t RMSE: 0.046\n",
      "alpha : 2.780 \t RMSE: 0.046\n",
      "alpha : 2.790 \t RMSE: 0.046\n",
      "alpha : 2.800 \t RMSE: 0.046\n",
      "alpha : 2.810 \t RMSE: 0.046\n",
      "alpha : 2.820 \t RMSE: 0.047\n",
      "alpha : 2.830 \t RMSE: 0.047\n",
      "alpha : 2.840 \t RMSE: 0.047\n",
      "alpha : 2.850 \t RMSE: 0.047\n",
      "alpha : 2.860 \t RMSE: 0.047\n",
      "alpha : 2.870 \t RMSE: 0.047\n",
      "alpha : 2.880 \t RMSE: 0.047\n",
      "alpha : 2.890 \t RMSE: 0.047\n",
      "alpha : 2.900 \t RMSE: 0.047\n",
      "alpha : 2.910 \t RMSE: 0.047\n",
      "alpha : 2.920 \t RMSE: 0.047\n",
      "alpha : 2.930 \t RMSE: 0.047\n",
      "alpha : 2.940 \t RMSE: 0.047\n",
      "alpha : 2.950 \t RMSE: 0.047\n",
      "alpha : 2.960 \t RMSE: 0.047\n",
      "alpha : 2.970 \t RMSE: 0.048\n",
      "alpha : 2.980 \t RMSE: 0.048\n",
      "alpha : 2.990 \t RMSE: 0.048\n"
     ]
    }
   ],
   "source": [
    "# you should not have imported sklearn before this point\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import absolute\n",
    "\n",
    "for i in np.arange(0,3,0.01):\n",
    "    aph = i\n",
    "    \n",
    "    model = Ridge(alpha=aph)\n",
    "    cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, x_train, y_train, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "    scores = absolute(scores)\n",
    "    print('alpha : %.3f \\t RMSE: %.3f' % (aph, std(scores)))\n",
    "# implement Ridge regression and make a table where you explore the effect of different values of `alpha`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "566d6090",
   "metadata": {},
   "source": [
    "alpha is a hyperparameter, default value is 1.0\n",
    "here i am taking values of alpha from 0 to 3 with a separation of 0.01.\n",
    "here alpha = 1.1 is giving best RMSE result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-temperature",
   "metadata": {},
   "source": [
    "### 2.3 Implement Lasso regression\n",
    "2.3.1 Explain Lasso regression briefly in 1-2 lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa12b0f",
   "metadata": {},
   "source": [
    "Lasso regression is a regularized LR model which includes L1 panality.lasso regression shrinks the cofficient of those input variables that do not contribute much in the task of the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "popular-wonder",
   "metadata": {},
   "source": [
    "2.3.2 Implement Lasso regression and make a table of different RMSE scores you achieved with different values of alpha. What does the parameter `alpha` do? How does it affect the results here? Explain in 5-10 lines in total."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89f430a",
   "metadata": {},
   "source": [
    "alpha is a hyperparameter, default value is 1.0\n",
    "here i am taking values of alpha from 0 to 3 with a separation of 0.01.\n",
    "here alpha =0.02 is giving best RMSE result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "extra-brighton",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha : 0.000 \t RMSE: 0.024\n",
      "alpha : 0.010 \t RMSE: 0.021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.516e-01, tolerance: 1.663e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.531e-01, tolerance: 1.784e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.028e-01, tolerance: 1.666e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.455e-01, tolerance: 1.748e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.408e-01, tolerance: 1.678e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.127e-01, tolerance: 1.753e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.515e-01, tolerance: 1.787e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.469e-01, tolerance: 1.709e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.526e-01, tolerance: 1.775e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.494e-01, tolerance: 1.703e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.364e-01, tolerance: 1.768e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.358e-01, tolerance: 1.759e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.413e-01, tolerance: 1.801e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.497e-01, tolerance: 1.632e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.513e-01, tolerance: 1.628e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.236e-01, tolerance: 1.713e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.226e-01, tolerance: 1.744e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.518e-01, tolerance: 1.725e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.518e-01, tolerance: 1.760e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.336e-01, tolerance: 1.608e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.501e-01, tolerance: 1.736e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.491e-01, tolerance: 1.796e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.284e-01, tolerance: 1.771e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.450e-01, tolerance: 1.793e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.457e-01, tolerance: 1.629e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.551e-01, tolerance: 1.530e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.493e-01, tolerance: 1.657e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.279e-01, tolerance: 1.825e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.541e-01, tolerance: 1.812e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:680: UserWarning: With alpha=0, this algorithm does not converge well. You are advised to use the LinearRegression estimator\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: UserWarning: Coordinate descent with no regularization may lead to unexpected results and is discouraged.\n",
      "  model = cd_fast.enet_coordinate_descent(\n",
      "/home/aman/.local/lib/python3.8/site-packages/sklearn/linear_model/_coordinate_descent.py:647: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.239e-01, tolerance: 1.830e-03 Linear regression models with null weight for the l1 regularization term are more efficiently fitted using one of the solvers implemented in sklearn.linear_model.Ridge/RidgeCV instead.\n",
      "  model = cd_fast.enet_coordinate_descent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha : 0.020 \t RMSE: 0.027\n",
      "alpha : 0.030 \t RMSE: 0.034\n",
      "alpha : 0.040 \t RMSE: 0.042\n",
      "alpha : 0.050 \t RMSE: 0.045\n",
      "alpha : 0.060 \t RMSE: 0.051\n",
      "alpha : 0.070 \t RMSE: 0.060\n",
      "alpha : 0.080 \t RMSE: 0.070\n",
      "alpha : 0.090 \t RMSE: 0.081\n",
      "alpha : 0.100 \t RMSE: 0.088\n",
      "alpha : 0.110 \t RMSE: 0.087\n",
      "alpha : 0.120 \t RMSE: 0.081\n",
      "alpha : 0.130 \t RMSE: 0.081\n",
      "alpha : 0.140 \t RMSE: 0.081\n",
      "alpha : 0.150 \t RMSE: 0.081\n",
      "alpha : 0.160 \t RMSE: 0.081\n",
      "alpha : 0.170 \t RMSE: 0.081\n",
      "alpha : 0.180 \t RMSE: 0.081\n",
      "alpha : 0.190 \t RMSE: 0.081\n",
      "alpha : 0.200 \t RMSE: 0.081\n",
      "alpha : 0.210 \t RMSE: 0.081\n",
      "alpha : 0.220 \t RMSE: 0.081\n",
      "alpha : 0.230 \t RMSE: 0.081\n",
      "alpha : 0.240 \t RMSE: 0.081\n",
      "alpha : 0.250 \t RMSE: 0.081\n",
      "alpha : 0.260 \t RMSE: 0.081\n",
      "alpha : 0.270 \t RMSE: 0.081\n",
      "alpha : 0.280 \t RMSE: 0.081\n",
      "alpha : 0.290 \t RMSE: 0.081\n",
      "alpha : 0.300 \t RMSE: 0.081\n",
      "alpha : 0.310 \t RMSE: 0.081\n",
      "alpha : 0.320 \t RMSE: 0.081\n",
      "alpha : 0.330 \t RMSE: 0.081\n",
      "alpha : 0.340 \t RMSE: 0.081\n",
      "alpha : 0.350 \t RMSE: 0.081\n",
      "alpha : 0.360 \t RMSE: 0.081\n",
      "alpha : 0.370 \t RMSE: 0.081\n",
      "alpha : 0.380 \t RMSE: 0.081\n",
      "alpha : 0.390 \t RMSE: 0.081\n",
      "alpha : 0.400 \t RMSE: 0.081\n",
      "alpha : 0.410 \t RMSE: 0.081\n",
      "alpha : 0.420 \t RMSE: 0.081\n",
      "alpha : 0.430 \t RMSE: 0.081\n",
      "alpha : 0.440 \t RMSE: 0.081\n",
      "alpha : 0.450 \t RMSE: 0.081\n",
      "alpha : 0.460 \t RMSE: 0.081\n",
      "alpha : 0.470 \t RMSE: 0.081\n",
      "alpha : 0.480 \t RMSE: 0.081\n",
      "alpha : 0.490 \t RMSE: 0.081\n",
      "alpha : 0.500 \t RMSE: 0.081\n",
      "alpha : 0.510 \t RMSE: 0.081\n",
      "alpha : 0.520 \t RMSE: 0.081\n",
      "alpha : 0.530 \t RMSE: 0.081\n",
      "alpha : 0.540 \t RMSE: 0.081\n",
      "alpha : 0.550 \t RMSE: 0.081\n",
      "alpha : 0.560 \t RMSE: 0.081\n",
      "alpha : 0.570 \t RMSE: 0.081\n",
      "alpha : 0.580 \t RMSE: 0.081\n",
      "alpha : 0.590 \t RMSE: 0.081\n",
      "alpha : 0.600 \t RMSE: 0.081\n",
      "alpha : 0.610 \t RMSE: 0.081\n",
      "alpha : 0.620 \t RMSE: 0.081\n",
      "alpha : 0.630 \t RMSE: 0.081\n",
      "alpha : 0.640 \t RMSE: 0.081\n",
      "alpha : 0.650 \t RMSE: 0.081\n",
      "alpha : 0.660 \t RMSE: 0.081\n",
      "alpha : 0.670 \t RMSE: 0.081\n",
      "alpha : 0.680 \t RMSE: 0.081\n",
      "alpha : 0.690 \t RMSE: 0.081\n",
      "alpha : 0.700 \t RMSE: 0.081\n",
      "alpha : 0.710 \t RMSE: 0.081\n",
      "alpha : 0.720 \t RMSE: 0.081\n",
      "alpha : 0.730 \t RMSE: 0.081\n",
      "alpha : 0.740 \t RMSE: 0.081\n",
      "alpha : 0.750 \t RMSE: 0.081\n",
      "alpha : 0.760 \t RMSE: 0.081\n",
      "alpha : 0.770 \t RMSE: 0.081\n",
      "alpha : 0.780 \t RMSE: 0.081\n",
      "alpha : 0.790 \t RMSE: 0.081\n",
      "alpha : 0.800 \t RMSE: 0.081\n",
      "alpha : 0.810 \t RMSE: 0.081\n",
      "alpha : 0.820 \t RMSE: 0.081\n",
      "alpha : 0.830 \t RMSE: 0.081\n",
      "alpha : 0.840 \t RMSE: 0.081\n",
      "alpha : 0.850 \t RMSE: 0.081\n",
      "alpha : 0.860 \t RMSE: 0.081\n",
      "alpha : 0.870 \t RMSE: 0.081\n",
      "alpha : 0.880 \t RMSE: 0.081\n",
      "alpha : 0.890 \t RMSE: 0.081\n",
      "alpha : 0.900 \t RMSE: 0.081\n",
      "alpha : 0.910 \t RMSE: 0.081\n",
      "alpha : 0.920 \t RMSE: 0.081\n",
      "alpha : 0.930 \t RMSE: 0.081\n",
      "alpha : 0.940 \t RMSE: 0.081\n",
      "alpha : 0.950 \t RMSE: 0.081\n",
      "alpha : 0.960 \t RMSE: 0.081\n",
      "alpha : 0.970 \t RMSE: 0.081\n",
      "alpha : 0.980 \t RMSE: 0.081\n",
      "alpha : 0.990 \t RMSE: 0.081\n",
      "alpha : 1.000 \t RMSE: 0.081\n",
      "alpha : 1.010 \t RMSE: 0.081\n",
      "alpha : 1.020 \t RMSE: 0.081\n",
      "alpha : 1.030 \t RMSE: 0.081\n",
      "alpha : 1.040 \t RMSE: 0.081\n",
      "alpha : 1.050 \t RMSE: 0.081\n",
      "alpha : 1.060 \t RMSE: 0.081\n",
      "alpha : 1.070 \t RMSE: 0.081\n",
      "alpha : 1.080 \t RMSE: 0.081\n",
      "alpha : 1.090 \t RMSE: 0.081\n",
      "alpha : 1.100 \t RMSE: 0.081\n",
      "alpha : 1.110 \t RMSE: 0.081\n",
      "alpha : 1.120 \t RMSE: 0.081\n",
      "alpha : 1.130 \t RMSE: 0.081\n",
      "alpha : 1.140 \t RMSE: 0.081\n",
      "alpha : 1.150 \t RMSE: 0.081\n",
      "alpha : 1.160 \t RMSE: 0.081\n",
      "alpha : 1.170 \t RMSE: 0.081\n",
      "alpha : 1.180 \t RMSE: 0.081\n",
      "alpha : 1.190 \t RMSE: 0.081\n",
      "alpha : 1.200 \t RMSE: 0.081\n",
      "alpha : 1.210 \t RMSE: 0.081\n",
      "alpha : 1.220 \t RMSE: 0.081\n",
      "alpha : 1.230 \t RMSE: 0.081\n",
      "alpha : 1.240 \t RMSE: 0.081\n",
      "alpha : 1.250 \t RMSE: 0.081\n",
      "alpha : 1.260 \t RMSE: 0.081\n",
      "alpha : 1.270 \t RMSE: 0.081\n",
      "alpha : 1.280 \t RMSE: 0.081\n",
      "alpha : 1.290 \t RMSE: 0.081\n",
      "alpha : 1.300 \t RMSE: 0.081\n",
      "alpha : 1.310 \t RMSE: 0.081\n",
      "alpha : 1.320 \t RMSE: 0.081\n",
      "alpha : 1.330 \t RMSE: 0.081\n",
      "alpha : 1.340 \t RMSE: 0.081\n",
      "alpha : 1.350 \t RMSE: 0.081\n",
      "alpha : 1.360 \t RMSE: 0.081\n",
      "alpha : 1.370 \t RMSE: 0.081\n",
      "alpha : 1.380 \t RMSE: 0.081\n",
      "alpha : 1.390 \t RMSE: 0.081\n",
      "alpha : 1.400 \t RMSE: 0.081\n",
      "alpha : 1.410 \t RMSE: 0.081\n",
      "alpha : 1.420 \t RMSE: 0.081\n",
      "alpha : 1.430 \t RMSE: 0.081\n",
      "alpha : 1.440 \t RMSE: 0.081\n",
      "alpha : 1.450 \t RMSE: 0.081\n",
      "alpha : 1.460 \t RMSE: 0.081\n",
      "alpha : 1.470 \t RMSE: 0.081\n",
      "alpha : 1.480 \t RMSE: 0.081\n",
      "alpha : 1.490 \t RMSE: 0.081\n",
      "alpha : 1.500 \t RMSE: 0.081\n",
      "alpha : 1.510 \t RMSE: 0.081\n",
      "alpha : 1.520 \t RMSE: 0.081\n",
      "alpha : 1.530 \t RMSE: 0.081\n",
      "alpha : 1.540 \t RMSE: 0.081\n",
      "alpha : 1.550 \t RMSE: 0.081\n",
      "alpha : 1.560 \t RMSE: 0.081\n",
      "alpha : 1.570 \t RMSE: 0.081\n",
      "alpha : 1.580 \t RMSE: 0.081\n",
      "alpha : 1.590 \t RMSE: 0.081\n",
      "alpha : 1.600 \t RMSE: 0.081\n",
      "alpha : 1.610 \t RMSE: 0.081\n",
      "alpha : 1.620 \t RMSE: 0.081\n",
      "alpha : 1.630 \t RMSE: 0.081\n",
      "alpha : 1.640 \t RMSE: 0.081\n",
      "alpha : 1.650 \t RMSE: 0.081\n",
      "alpha : 1.660 \t RMSE: 0.081\n",
      "alpha : 1.670 \t RMSE: 0.081\n",
      "alpha : 1.680 \t RMSE: 0.081\n",
      "alpha : 1.690 \t RMSE: 0.081\n",
      "alpha : 1.700 \t RMSE: 0.081\n",
      "alpha : 1.710 \t RMSE: 0.081\n",
      "alpha : 1.720 \t RMSE: 0.081\n",
      "alpha : 1.730 \t RMSE: 0.081\n",
      "alpha : 1.740 \t RMSE: 0.081\n",
      "alpha : 1.750 \t RMSE: 0.081\n",
      "alpha : 1.760 \t RMSE: 0.081\n",
      "alpha : 1.770 \t RMSE: 0.081\n",
      "alpha : 1.780 \t RMSE: 0.081\n",
      "alpha : 1.790 \t RMSE: 0.081\n",
      "alpha : 1.800 \t RMSE: 0.081\n",
      "alpha : 1.810 \t RMSE: 0.081\n",
      "alpha : 1.820 \t RMSE: 0.081\n",
      "alpha : 1.830 \t RMSE: 0.081\n",
      "alpha : 1.840 \t RMSE: 0.081\n",
      "alpha : 1.850 \t RMSE: 0.081\n",
      "alpha : 1.860 \t RMSE: 0.081\n",
      "alpha : 1.870 \t RMSE: 0.081\n",
      "alpha : 1.880 \t RMSE: 0.081\n",
      "alpha : 1.890 \t RMSE: 0.081\n",
      "alpha : 1.900 \t RMSE: 0.081\n",
      "alpha : 1.910 \t RMSE: 0.081\n",
      "alpha : 1.920 \t RMSE: 0.081\n",
      "alpha : 1.930 \t RMSE: 0.081\n",
      "alpha : 1.940 \t RMSE: 0.081\n",
      "alpha : 1.950 \t RMSE: 0.081\n",
      "alpha : 1.960 \t RMSE: 0.081\n",
      "alpha : 1.970 \t RMSE: 0.081\n",
      "alpha : 1.980 \t RMSE: 0.081\n",
      "alpha : 1.990 \t RMSE: 0.081\n",
      "alpha : 2.000 \t RMSE: 0.081\n",
      "alpha : 2.010 \t RMSE: 0.081\n",
      "alpha : 2.020 \t RMSE: 0.081\n",
      "alpha : 2.030 \t RMSE: 0.081\n",
      "alpha : 2.040 \t RMSE: 0.081\n",
      "alpha : 2.050 \t RMSE: 0.081\n",
      "alpha : 2.060 \t RMSE: 0.081\n",
      "alpha : 2.070 \t RMSE: 0.081\n",
      "alpha : 2.080 \t RMSE: 0.081\n",
      "alpha : 2.090 \t RMSE: 0.081\n",
      "alpha : 2.100 \t RMSE: 0.081\n",
      "alpha : 2.110 \t RMSE: 0.081\n",
      "alpha : 2.120 \t RMSE: 0.081\n",
      "alpha : 2.130 \t RMSE: 0.081\n",
      "alpha : 2.140 \t RMSE: 0.081\n",
      "alpha : 2.150 \t RMSE: 0.081\n",
      "alpha : 2.160 \t RMSE: 0.081\n",
      "alpha : 2.170 \t RMSE: 0.081\n",
      "alpha : 2.180 \t RMSE: 0.081\n",
      "alpha : 2.190 \t RMSE: 0.081\n",
      "alpha : 2.200 \t RMSE: 0.081\n",
      "alpha : 2.210 \t RMSE: 0.081\n",
      "alpha : 2.220 \t RMSE: 0.081\n",
      "alpha : 2.230 \t RMSE: 0.081\n",
      "alpha : 2.240 \t RMSE: 0.081\n",
      "alpha : 2.250 \t RMSE: 0.081\n",
      "alpha : 2.260 \t RMSE: 0.081\n",
      "alpha : 2.270 \t RMSE: 0.081\n",
      "alpha : 2.280 \t RMSE: 0.081\n",
      "alpha : 2.290 \t RMSE: 0.081\n",
      "alpha : 2.300 \t RMSE: 0.081\n",
      "alpha : 2.310 \t RMSE: 0.081\n",
      "alpha : 2.320 \t RMSE: 0.081\n",
      "alpha : 2.330 \t RMSE: 0.081\n",
      "alpha : 2.340 \t RMSE: 0.081\n",
      "alpha : 2.350 \t RMSE: 0.081\n",
      "alpha : 2.360 \t RMSE: 0.081\n",
      "alpha : 2.370 \t RMSE: 0.081\n",
      "alpha : 2.380 \t RMSE: 0.081\n",
      "alpha : 2.390 \t RMSE: 0.081\n",
      "alpha : 2.400 \t RMSE: 0.081\n",
      "alpha : 2.410 \t RMSE: 0.081\n",
      "alpha : 2.420 \t RMSE: 0.081\n",
      "alpha : 2.430 \t RMSE: 0.081\n",
      "alpha : 2.440 \t RMSE: 0.081\n",
      "alpha : 2.450 \t RMSE: 0.081\n",
      "alpha : 2.460 \t RMSE: 0.081\n",
      "alpha : 2.470 \t RMSE: 0.081\n",
      "alpha : 2.480 \t RMSE: 0.081\n",
      "alpha : 2.490 \t RMSE: 0.081\n",
      "alpha : 2.500 \t RMSE: 0.081\n",
      "alpha : 2.510 \t RMSE: 0.081\n",
      "alpha : 2.520 \t RMSE: 0.081\n",
      "alpha : 2.530 \t RMSE: 0.081\n",
      "alpha : 2.540 \t RMSE: 0.081\n",
      "alpha : 2.550 \t RMSE: 0.081\n",
      "alpha : 2.560 \t RMSE: 0.081\n",
      "alpha : 2.570 \t RMSE: 0.081\n",
      "alpha : 2.580 \t RMSE: 0.081\n",
      "alpha : 2.590 \t RMSE: 0.081\n",
      "alpha : 2.600 \t RMSE: 0.081\n",
      "alpha : 2.610 \t RMSE: 0.081\n",
      "alpha : 2.620 \t RMSE: 0.081\n",
      "alpha : 2.630 \t RMSE: 0.081\n",
      "alpha : 2.640 \t RMSE: 0.081\n",
      "alpha : 2.650 \t RMSE: 0.081\n",
      "alpha : 2.660 \t RMSE: 0.081\n",
      "alpha : 2.670 \t RMSE: 0.081\n",
      "alpha : 2.680 \t RMSE: 0.081\n",
      "alpha : 2.690 \t RMSE: 0.081\n",
      "alpha : 2.700 \t RMSE: 0.081\n",
      "alpha : 2.710 \t RMSE: 0.081\n",
      "alpha : 2.720 \t RMSE: 0.081\n",
      "alpha : 2.730 \t RMSE: 0.081\n",
      "alpha : 2.740 \t RMSE: 0.081\n",
      "alpha : 2.750 \t RMSE: 0.081\n",
      "alpha : 2.760 \t RMSE: 0.081\n",
      "alpha : 2.770 \t RMSE: 0.081\n",
      "alpha : 2.780 \t RMSE: 0.081\n",
      "alpha : 2.790 \t RMSE: 0.081\n",
      "alpha : 2.800 \t RMSE: 0.081\n",
      "alpha : 2.810 \t RMSE: 0.081\n",
      "alpha : 2.820 \t RMSE: 0.081\n",
      "alpha : 2.830 \t RMSE: 0.081\n",
      "alpha : 2.840 \t RMSE: 0.081\n",
      "alpha : 2.850 \t RMSE: 0.081\n",
      "alpha : 2.860 \t RMSE: 0.081\n",
      "alpha : 2.870 \t RMSE: 0.081\n",
      "alpha : 2.880 \t RMSE: 0.081\n",
      "alpha : 2.890 \t RMSE: 0.081\n",
      "alpha : 2.900 \t RMSE: 0.081\n",
      "alpha : 2.910 \t RMSE: 0.081\n",
      "alpha : 2.920 \t RMSE: 0.081\n",
      "alpha : 2.930 \t RMSE: 0.081\n",
      "alpha : 2.940 \t RMSE: 0.081\n",
      "alpha : 2.950 \t RMSE: 0.081\n",
      "alpha : 2.960 \t RMSE: 0.081\n",
      "alpha : 2.970 \t RMSE: 0.081\n",
      "alpha : 2.980 \t RMSE: 0.081\n",
      "alpha : 2.990 \t RMSE: 0.081\n"
     ]
    }
   ],
   "source": [
    "# implement Lasso regression and make a table where you explore the effect of different values of `alpha`\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import absolute\n",
    "\n",
    "for i in np.arange(0,3,0.01):\n",
    "    # aph = random.randint(0,100)\n",
    "    aph=i\n",
    "\n",
    "    model = Lasso(alpha=aph)\n",
    "    # cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(model, x_train, y_train, scoring='neg_root_mean_squared_error', cv=cv, n_jobs=-1)\n",
    "    scores = absolute(scores)\n",
    "    print('alpha : %.3f \\t RMSE: %.3f' % (aph, std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9510b742",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accompanied-worst",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
